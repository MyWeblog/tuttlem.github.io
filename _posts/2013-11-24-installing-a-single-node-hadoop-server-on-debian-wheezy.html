---
layout: post
title: Installing a Single Node Hadoop Server on Debian Wheezy
date: 2013-11-24
comments: false
---

<h1>{{ page.title }}</h1>
<div class='post'>
Today's post is just a run sheet of steps to take to get a single-node <a href="http://hadoop.apache.org/">Hadoop</a> server up and running on <a href="https://wiki.debian.org/DebianWheezy">Debian Wheezy</a>. Keep in mind, this guide is for the 1.x.x series release of Hadoop.<br /><br />1. Install java 6 jdk (<a href="http://ddmytrenko.blogspot.com.au/2012/02/installing-sun-java-6-jdk-on-debian.html">http://ddmytrenko.blogspot.com.au/2012/02/installing-sun-java-6-jdk-on-debian.html</a>)<br /><br />Add a line to apt sources for "squeeze non-free"<br /><br /><pre class="brush: shell">$ sudo apt-get update<br />$ sudo apt-get install sun-java6-jdk<br />$ sudo update-java-alternatives -s java-6-sun<br /></pre><br />2. Add a haddop user<br /><br /><pre class="brush: shell">$ sudo addgroup hadoop<br />$ sudo adduser --ingroup hadoop hduser<br /></pre><br />3. Disable ipv6: add the following to the end of /etc/sysctl.conf<br /><br /><pre class="brush: plain">#disable ipv6<br />net.ipv6.conf.all.disable_ipv6 = 1<br />net.ipv6.conf.default.disable_ipv6 = 1<br />net.ipv6.conf.lo.disable_ipv6 = 1<br /></pre><br />4. Add the following to ~/.bashrc<br /><br /><pre class="brush:plain"># Set Hadoop-related environment variables<br />export HADOOP_HOME=/opt/hadoop<br /><br /># Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on)<br />export JAVA_HOME=/opt/jvm/jdk1.6.0_38<br /><br /># Add Hadoop bin/ and JAVA bin/ directory to PATH<br />export PATH=$PATH:$HADOOP_HOME/bin<br />export PATH=$PATH:$JAVA_HOME/bin<br /></pre><br />5. Download the "bin" flavour from here: <a href="http://apache.mirror.uber.com.au/hadoop/common/stable/">http://apache.mirror.uber.com.au/hadoop/common/stable/</a> extract it, move it into /opt under the name "hadoop"<br /><br />6. hadoop-env.sh<br /><br />Open opt/hadoop/conf/hadoop-env.sh and set the JAVA_HOME environment variable to the Sun JDK/JRE 6 directory.<br /><br /><pre class="brush:plain"># The java implementation to use.&nbsp; Required.<br /># export JAVA_HOME=/usr/lib/j2sdk1.5-sun<br />to<br />export JAVA_HOME=/opt/lib/jvm/java-6-sun<br /></pre><br />7. core-site.xml<br /><br />This is where Hadoop stores its Data.<br />/opt/hadoop/conf/core-site.xml<br /><br /><pre class="brush:plain">&lt;!-- In: conf/core-site.xml --&gt;<br />&lt;!-- In: conf/core-site.xml --&gt;<br />&lt;property&gt;<br />&nbsp; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br />&nbsp; &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;<br />&nbsp; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;<br />&lt;/property&gt;<br /><br />&lt;property&gt;<br />&nbsp; &lt;name&gt;fs.default.name&lt;/name&gt;<br />&nbsp; &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;<br />&nbsp; &lt;description&gt;The name of the default file system.&nbsp; A URI whose<br />&nbsp; scheme and authority determine the FileSystem implementation.&nbsp; The<br />&nbsp; uri's scheme determines the config property (fs.SCHEME.impl) naming<br />&nbsp; the FileSystem implementation class.&nbsp; The uri's authority is used to<br />&nbsp; determine the host, port, etc. for a filesystem.&lt;/description&gt;<br />&lt;/property&gt;<br /></pre>We need to create this directory and set ownership correctly:<br /><br /><pre class="brush:shell">$ mkdir -p /app/hadoop/tmp<br />$ chown hduser:hadoop /app/hadoop/tmp<br /></pre><br />8. mapred-site.xml<br /><br />vim mapred-site.xml<br /><br /><pre class="brush:plain">&lt;!-- In: conf/mapred-site.xml --&gt;<br />&lt;property&gt;<br />&nbsp; &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br />&nbsp; &lt;value&gt;localhost:54311&lt;/value&gt;<br />&nbsp; &lt;description&gt;The host and port that the MapReduce job tracker runs<br />&nbsp; at.&nbsp; If "local", then jobs are run in-process as a single map<br />&nbsp; and reduce task.<br />&nbsp; &lt;/description&gt;<br />&lt;/property&gt;<br /></pre><br />9. hdfs-site.xml<br /><br />vim hdfs-site.xml<br /><br /><pre class="brush:plain">&lt;!-- In: conf/hdfs-site.xml --&gt;<br />&lt;property&gt;<br />&nbsp; &lt;name&gt;dfs.replication&lt;/name&gt;<br />&nbsp; &lt;value&gt;1&lt;/value&gt;<br />&nbsp; &lt;description&gt;Default block replication.<br />&nbsp; The actual number of replications can be specified when the file is created.<br />&nbsp; The default is used if replication is not specified in create time.<br />&nbsp; &lt;/description&gt;<br />&lt;/property&gt;<br /></pre><br />10. Starting Hadoop (as hduser)<br /><br />Format the namenode<br /><pre class="brush:shell">$ /opt/hadoop/bin/hadoop namenode -format</pre><br />Start hadoop<br /><pre class="brush:shell">$ /opt/hadoop/bin/start-all.sh</pre><br />Up and running:<br /><br />NameNode daemon<br /><a href="http://localhost:50070/">http://localhost:50070/</a><br /><br />JobTracker daemon<br /><a href="http://localhost:50030/">http://localhost:50030/</a><br /><br />TaskTracker daemon<br /><a href="http://localhost:50060/">http://localhost:50060/</a><br /><br /></div>
<h2>Comments</h2>
<div class='comments'>
</div>
